{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam,rmsprop\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Convolution2D as Conv2D\n",
    "from keras.layers.convolutional import Deconv2D as Conv2DTranspose\n",
    "from keras.layers import Lambda, Input, Dense, MaxPooling2D, BatchNormalization,Input\n",
    "from keras.layers import UpSampling2D, Dropout, Flatten, Reshape, RepeatVector, LeakyReLU,Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "keras.callbacks.TerminateOnNaN()\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto( device_count = {'GPU': 4} ) \n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"#Setting the script to run on GPU:1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# path to USB\n",
    "#USBPath = \"/home/scope/Shreyas/conferences/NoveltyDetection/Data/WAAS/Train/\"\n",
    "USBPath = 'mnist_png\\\\training\\\\'\n",
    "\n",
    "# list of folders used in training\n",
    "#trainingFolders = [\"clear-day\",\"clear-day-50\",\"soft-rain\",\"soft-rain-50\",\"mid-rain\",\"mid-rain-50\"]\n",
    "trainingFolders = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "\n",
    "#Load complete input images without shuffling\n",
    "def load_images(paths):\n",
    "    numImages = 0\n",
    "    inputs = []\n",
    "    for path in paths:\n",
    "        numFiles = len(glob.glob1(path,'*.png'))\n",
    "        numImages += numFiles\n",
    "        for img in glob.glob(path+'*.png'):\n",
    "            img = cv2.imread(img)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = img / 255.\n",
    "            inputs.append(img)\n",
    "    #inpu = shuffle(inputs)\n",
    "    print(\"Total number of images:%d\" %(numImages))\n",
    "    return inputs\n",
    "\n",
    "def createFolderPaths(folders):\n",
    "    paths = []\n",
    "    for folder in folders:\n",
    "        #path = USBPath + folder + '/'\n",
    "        path = USBPath + folder + '\\\\'\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def load_training_images():\n",
    "    paths = createFolderPaths(trainingFolders)\n",
    "    return load_images(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:6009\n"
     ]
    }
   ],
   "source": [
    "#Loading images from the datasets\n",
    "csv_input = load_training_images()\n",
    "len(csv_input)#length of the data\n",
    "csv_input = shuffle(csv_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5809, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data to train and test. Then shuffle it for training\n",
    "from sklearn.utils import shuffle\n",
    "img_train, img_test = np.array(csv_input[0:len(csv_input)-200].copy()), np.array(csv_input[len(csv_input)-200:len(csv_input)].copy())\n",
    "img_train = np.reshape(img_train, [-1, img_train.shape[1],img_train.shape[2],img_train.shape[3]])\n",
    "img_test = np.reshape(img_test, [-1, img_test.shape[1],img_test.shape[2],img_test.shape[3]])\n",
    "#Shuffle the data in order to get different images in train and test datasets.\n",
    "#img_train = shuffle(img_train)\n",
    "#img_test = shuffle(img_test)\n",
    "inp = (img_train, img_test)\n",
    "img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Only parameters that has to be changed\n",
    "#Working_directory = \"/home/scope/Shreyas/conferences/NoveltyDetection/Code/Carla/Experiments/ISORC/\"#working directory\n",
    "#Working_folder = 'B_1.5_L_30_mid'#experiment\n",
    "#Working_path = Working_directory + Working_folder + '\\\\'\n",
    "\n",
    "Working_directory = 'experiment\\\\'#working directory\n",
    "Working_folder = 'B_1.5_L_30_mid'#experiment\n",
    "Working_path = Working_directory + Working_folder + '\\\\'\n",
    "\n",
    "trainfolder = 'train_reconstruction_result'#train folder\n",
    "data = CSVLogger(Working_path + '\\\\kerasloss.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Sampling function used by the VAE\n",
    "def sample_func(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#CNN-VAE model. Only important part is the N_latent variable which holds the latent space data.\n",
    "def CreateModels(n_latent=30, sample_enc=sample_func, beta=1.5, C=0):\n",
    "    model = Sequential()\n",
    "    input_img = Input(shape=(224,224,3), name='image')\n",
    "    x = Conv2D(128, (3, 3),  use_bias=False, padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(1000)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(250)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "#     x = Dense(50)(x)\n",
    "#     x = LeakyReLU(0.1)(x)\n",
    "    \n",
    "    z_mean = Dense(n_latent, name='z_mean')(x)\n",
    "    z_log_var = Dense(n_latent, name='z_log_var')(x)\n",
    "    z = Lambda(sample_func, output_shape=(n_latent,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(input_img, [z_mean, z_log_var, z], name='encoder')\n",
    "    #encoder.summary()\n",
    "    \n",
    "    latent_inputs = Input(shape=(n_latent,), name='z_sampling')\n",
    "#     x = Dense(50)(latent_inputs)\n",
    "#     x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(250)(latent_inputs)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(1000)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(3136)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = Reshape((14, 14, 16))(x)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(3, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    decoded = Activation('sigmoid')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, decoded)\n",
    "    #decoder.summary()\n",
    "\n",
    "    outputs = decoder(encoder(input_img)[2])\n",
    "    autoencoder = Model(input_img,outputs)\n",
    "    #autoencoder.summary()\n",
    "    \n",
    "    def vae_loss(true, pred):\n",
    "        rec_loss = mse(K.flatten(true), K.flatten(pred))\n",
    "        rec_loss *= 224*224*3\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(rec_loss + beta*(kl_loss-C))\n",
    "        return vae_loss\n",
    "        #autoencoder.add_loss(vae_loss)\n",
    "    #Define adam optimizer\n",
    "    adam = keras.optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #autoencoder.compile(optimizer='rmsprop',loss=vae_loss, metrics=[vae_loss])\n",
    "    autoencoder.compile(optimizer='adam',loss=vae_loss, metrics=[vae_loss])\n",
    "\n",
    "    return autoencoder, encoder,decoder, z_log_var\n",
    "\n",
    "\n",
    "#Train function to fit the data to the model\n",
    "def train(X,autoencoder):\n",
    "    X_train,X_test = X\n",
    "    filePath = Working_path + 'weights.best.hdf5'#checkpoint weights\n",
    "    print('1')\n",
    "    checkpoint = ModelCheckpoint(filePath, monitor='vae_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    print('2')\n",
    "    EarlyStopping(monitor='vae_loss', patience=5, verbose=0),\n",
    "    #es=EarlyStopping(monitor='vae_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    callbacks_list = [checkpoint, data]\n",
    "    print('3')\n",
    "    autoencoder.fit(X_train, X_train,epochs=10,batch_size=10,shuffle=True,validation_data=(X_test, X_test),callbacks=callbacks_list, verbose=2)\n",
    "    print('4')\n",
    "    \n",
    "    #Save the autoencoder model\n",
    "def SaveAutoencoderModel(autoencoder):\n",
    "\tauto_model_json = autoencoder.to_json()\n",
    "\twith open(Working_path + 'auto_model.json', \"w\") as json_file:\n",
    "\t\tjson_file.write(auto_model_json)\n",
    "\tautoencoder.save_weights(Working_path + 'auto_model.h5')\n",
    "\tprint(\"Saved Autoencoder model to disk\")\n",
    "\n",
    "#Save the encoder model\n",
    "def SaveEncoderModel(encoder):\n",
    "\ten_model_json = encoder.to_json()\n",
    "\twith open(Working_path + 'en_model.json', \"w\") as json_file:\n",
    "\t\tjson_file.write(en_model_json)\n",
    "\tencoder.save_weights(Working_path + 'en_model.h5')\n",
    "\tprint(\"Saved Encoder model to disk\")\n",
    "\n",
    "#Test the trained models on a different test data\n",
    "def test(autoencoder,encoder,test):\n",
    "    autoencoder_res = autoencoder.predict(test)\n",
    "    encoder_res = encoder.predict(test)\n",
    "    res_x = test.copy()\n",
    "    res_y = autoencoder_res.copy()\n",
    "    res_x = res_x * 255\n",
    "    res_y = res_y * 255\n",
    "\n",
    "    return res_x, res_y, encoder_res\n",
    "\n",
    "#Save the reconstructed test data in a separate folder. \n",
    "#For this create a folder named results in the directory you are working in.\n",
    "def savedata(test_in, test_out, test_encoded, Working_path, trainfolder):\n",
    "    os.makedirs(Working_path + trainfolder + '\\\\', exist_ok=True)\n",
    "    for i in range(len(test_in)):\n",
    "        test_in = np.reshape(test_in,[-1, 224,224,3])#Reshape the data\n",
    "        test_out = np.reshape(test_out,[-1, 224,224,3])#Reshape the data\n",
    "        cv2.imwrite(Working_path + trainfolder + '\\\\' + str(i) +'_in.png', test_in[i])\n",
    "        cv2.imwrite(Working_path + trainfolder + '\\\\' + str(i) +'_out.png', test_out[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder,encoder,decoder,z_log_var = CreateModels()# Running the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "Train on 5809 samples, validate on 200 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0eb5b38f3b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Train the model with the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-23df37a64c01>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, autoencoder)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\Py3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(inp,autoencoder)#Train the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_out, test_encoded = test(autoencoder, encoder, inp[1])#Test the trained model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedata(test_in, test_out, test_encoded, Working_path, trainfolder)#Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Encoder model to disk\n",
      "Saved Autoencoder model to disk\n"
     ]
    }
   ],
   "source": [
    "SaveEncoderModel(encoder)\n",
    "SaveAutoencoderModel(autoencoder)#Save the autoencoder and encoder models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
